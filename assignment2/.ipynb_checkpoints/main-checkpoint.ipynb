{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import random\n",
    "import nltk\n",
    "from operator import itemgetter\n",
    "import re\n",
    "import numpy as np\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def textProc(file_name):\n",
    "    f= open(file_name,'r')\n",
    "    #tp1\n",
    "    corpus = f.read().replace('\\n',' ') #replace the new line with single white space.\n",
    "    corpus = corpus.lower() #tp2\n",
    "\n",
    "    token = sent_tokenize(corpus)\n",
    "\n",
    "    #replace all the non alphanumeric characters with whitespace.\n",
    "    token = [re.sub(r'[^A-Za-z0-9]',\" \", i) for i in token]#tp3\n",
    "\n",
    "    random.shuffle(token)\n",
    "\n",
    "    #addtG(token) #add tags\n",
    "    return token\n",
    "\n",
    "    #clean tokens of sentence returned\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function for count of n-gram\n",
    "def LstOfNgram(count,corpus):\n",
    "    #cleaned corpus\n",
    "    n_gram=[]\n",
    "    for i in corpus:\n",
    "        p = word_tokenize(i)\n",
    "        p = [\"<s>\"]+p\n",
    "        p.append(\"</s>\")    \n",
    "        #token must have start and end line tags here.\n",
    "        #print (p)\n",
    "        for g in range(count-1,len(p)):\n",
    "            dummy=\"\"\n",
    "            for j in range(count):\n",
    "                dummy = p[g-j]+\" \"+dummy   #i,i-1,i-2,i-3,i-4,i-5,...,i-(n-1)\n",
    "               #more things to be added here\n",
    "            dummy = re.sub(r\"^\\s|\\s$\",\"\",dummy)\n",
    "            n_gram.append(dummy)\n",
    "    return (n_gram)    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def CountNgram(frequency):\n",
    "    count ={}\n",
    "    \n",
    "    for (key,fvalue) in reversed(sorted(frequency.items(), key = itemgetter(1))):\n",
    "        count[key] = fvalue\n",
    " \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#unified method for MLE of n-gram\n",
    "def MLE(a,b,N):  #n is N of n-gram\n",
    "    try:\n",
    "        if N ==1:#unigram\n",
    "            return (count_unigram[a]/len(unigramLst))\n",
    "        elif N==2:#bigram\n",
    "            return (count_bigram[b+\" \"+a]/count_unigram[b])\n",
    "        elif N == 3:#triram\n",
    "            return (count_trigram[b+\" \"+a]/count_bigram[b])\n",
    "        elif N == 4:\n",
    "            return (count_quadgram[b+\" \"+a]/count_trigram[b])\n",
    "    except:\n",
    "            return 0 #0 probability if not present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generative function for n-gram model. \n",
    "def generate(mle):#mle can be of bi gram, unigram, trgram or quadgram.\n",
    "    sentence = []\n",
    "    #first generate the <s>\n",
    "    #generate other words recursvely\n",
    "    #untill <\\s> is found\n",
    "    keys = list(mle.keys())\n",
    "    #normalize the MLEs\n",
    "    prob = np.array([mle[i] for i in keys])\n",
    "    prob = prob/prob.sum()\n",
    "    while (True):\n",
    "        newWord = np.random.choice(keys,1,replace = True,p=prob)\n",
    "        if \"<s>\" in newWord[0]:#<s> has to be at the begining of the n gram.\n",
    "            #start of word detected\n",
    "             break\n",
    "        else:\n",
    "            pass\n",
    "    sentence.append(newWord[0])\n",
    "    newWord = np.random.choice(keys,1,replace = True,p=prob)\n",
    "    while(True):\n",
    "        if \"</s>\" in newWord[0]:\n",
    "            #we have found the end of sentence\n",
    "            sentence.append(newWord[0])\n",
    "            break\n",
    "        else:\n",
    "            #just append to the list\n",
    "            sentence.append(newWord[0])\n",
    "        newWord = np.random.choice(keys,1,replace = True,p=prob)\n",
    "       \n",
    "    return (\" \".join(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def probLog(sentence,mle):\n",
    "    N = len(list(mle.keys())[0].split(\" \"))\n",
    "    tokenSent = sentence.split(\" \")\n",
    "    tokenSent.append(\"</s>\")\n",
    "    tokenSent = [\"<s>\"]+tokenSent\n",
    "    logProbBigram = 0\n",
    "   # prob = 1\n",
    "    #pass the word to text processing before tokenizing.\n",
    "    for i in range(N-1,len(tokenSent)):\n",
    "         p = tokenSent[i]\n",
    "         for j in range(i-1,i-(N),-1):\n",
    "            p =tokenSent[j]+\" \"+p\n",
    "        # prob = prob*mle[p]\n",
    "         logProbBigram += math.log(mle[p])\n",
    "    return (logProbBigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_NLst(count_dict):#pass dicationary of word vs count.\n",
    "    #create counting list for \n",
    "    l = count_dict \n",
    "    l=list(l.values())\n",
    "    l.sort()\n",
    "    #we want a dictionary with {1:1,2:1,3:2,4:3,5:1}\n",
    "    d={}\n",
    "    for i in range(len(l)):\n",
    "        count =1\n",
    "        for j in range(i,0,-1):\n",
    "            if l[i] == l[j-1]:\n",
    "                count = count + 1\n",
    "            else:\n",
    "                break \n",
    "    \n",
    "        d[l[i]] = count\n",
    "    return (d)\n",
    "#takes O(n^2) which takes lot of time to execute.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class add1_smoothing_bigram:#old_count is the dictionary of count versus bigram/unigram\n",
    "    \n",
    "    def __init__(self,old_countUnigram, old_countBigram):\n",
    "        self.old_countUnigram = old_countUnigram.copy()\n",
    "        self.old_countBigram = old_countBigram.copy()\n",
    "        self.new_countBigram = old_countBigram.copy()\n",
    "#        print (self.new_countBigram)\n",
    "        self.countBigram()\n",
    "        \n",
    "        \n",
    "    def countBigram(self):\n",
    "           \n",
    "            for i in self.new_countBigram.keys():\n",
    "                a_,b_= i.split(\" \")[1],i.split(\" \")[0]\n",
    "                self.new_countBigram[i]=self.old_countUnigram[b_]*self.Prob(a_,b_,1)\n",
    "                \n",
    "        \n",
    "    def Prob(self,a,b,countBigram): #P(a|b)\n",
    "            bigram = b+\" \"+a\n",
    "            unigram = b\n",
    "            try:\n",
    "                numerator = self.old_countBigram[bigram] +1\n",
    "            except :\n",
    "                numerator = 1\n",
    "            try:   \n",
    "                denominator = self.old_countUnigram[unigram] + len(self.old_countUnigram.keys()) #types\n",
    "            except :\n",
    "                denominator = len(self.old_countUnigram.keys())\n",
    "            return (numerator/denominator)\n",
    "#assure once that it is len(count_bigam)    #see if it needs to be replaced by token?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class goodTuring:\n",
    "    def __init__(self,Bcount,Ucount): #dictionary of unigram and bigram count\n",
    "        self.biGramCount = Bcount.copy()\n",
    "        self.UniGramCount = Ucount.copy()\n",
    "        \n",
    "        self.bidictN = get_NLst(self.biGramCount)\n",
    "        self.UnidictN = get_NLst(self.UniGramCount)\n",
    "        \n",
    "        self.keysbi,self.valuesbi = list(self.bidictN.keys()),list(self.bidictN.values())    #[N1,N2,N3,N4,N5,N6,...]\n",
    "        self.keysuni,self.valuesuni = list(self.UnidictN.keys()),list(self.UnidictN.values()) #count of N1,N2,N3,...\n",
    "        \n",
    "        self.NewCountbi()\n",
    "        self.NewCountuni()\n",
    "    def NewCountbi(self):\n",
    "               \n",
    "        for i in self.biGramCount.keys():\n",
    "            try:\n",
    "                self.biGramCount[i] = (self.biGramCount[i]+1)*(self.bidictN[self.biGramCount[i]+1]/self.bidictN[self.biGramCount[i]])\n",
    "            except:\n",
    "                intr= np.interp(float(self.biGramCount[i]),[float(j) for j in self.keysbi],[float(k) for k in self.valuesbi])\n",
    "                #intr= np.interp(l[i]+1,keys,values)\n",
    "\n",
    "                self.biGramCount[i] = (self.biGramCount[i]+1)*(intr/(self.bidictN[self.biGramCount[i]]))\n",
    "            #new for the word with zero count \n",
    "\n",
    "        return self.biGramCount \n",
    "    def NewCountuni(self):\n",
    "               \n",
    "        for i in self.UniGramCount.keys():\n",
    "            try:\n",
    "                self.UniGramCount[i] = (self.UniGramCount[i]+1)*(self.UnidictN[self.UniGramCount[i]+1]/self.UnidictN[self.UniGramCount[i]])\n",
    "            except:\n",
    "                intr= np.interp(float(self.UniGramCount[i]),[float(j) for j in self.keysuni],[float(k) for k in self.valuesuni])\n",
    "                #intr= np.interp(l[i]+1,keys,values)\n",
    "\n",
    "                self.UniGramCount[i] = (self.UniGramCount[i]+1)*(intr/(self.UnidictN[self.UniGramCount[i]]))\n",
    "            #new for the word with zero count \n",
    "\n",
    "        return self.UniGramCount \n",
    "    \n",
    "    def Prob(self,a,b, tokenBigram):   #total number of bygram needs to be passed here\n",
    "        #token : total number of token of n-gram \n",
    "        bigram = b+\" \"+a\n",
    "        if bigram not in self.biGramCount.keys():\n",
    "            \n",
    "            return (self.bidictN[1]/len(tokenBigram))\n",
    "        \n",
    "        return self.biGramCount[bigram]/self.UniGramCount[b]\n",
    "#look over interpolation function\n",
    "#for Nc >Nc+1 it is too jumpy\n",
    "#the above probability may be wrong??\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def perPlexity(data,bigramLst,model):\n",
    "    log_perplxty = 0\n",
    "    for i in data:#data is the list of all the sentences \n",
    "        #i is a sentence\n",
    "        log_prob = 0\n",
    "        a = LstOfNgram(2,[i])\n",
    "        for i in a:\n",
    "            new = model.Prob(i.split(\" \")[1],i.split(\" \")[0],bigramLst)\n",
    "            #print(new)\n",
    "            log_prob = log_prob + math.log(new)\n",
    "            \n",
    "        log_perplxty = log_perplxty+log_prob\n",
    "        log_perplxty = (-1/len(data))*(log_perplxty)\n",
    "       \n",
    "    return (math.exp(log_perplxty) )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usefull data generation from the word corpus.(Question 1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token = textProc('SherlockHomes.txt')\n",
    "random.shuffle(token)\n",
    "\n",
    "train = token[:int(len(token)*0.8)]#80%\n",
    "test= token[int(len(token)*0.8):]#20%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#n-gram count\n",
    "\n",
    "#list contining n-gram\n",
    "unigramLst = LstOfNgram(1,train)  ##the first number here representes the N of N-gram.\n",
    "bigramLst  = LstOfNgram(2,train)\n",
    "trigramLst = LstOfNgram(3,train)\n",
    "quadgramLst= LstOfNgram(4,train)\n",
    "#print (unigramLst)\n",
    "#  query for count of unigram only once\n",
    "feq  =  nltk.FreqDist(unigramLst)\n",
    "freq =  nltk.FreqDist(bigramLst)\n",
    "fre  =  nltk.FreqDist(trigramLst)\n",
    "fr   =  nltk.FreqDist(quadgramLst)\n",
    "\n",
    "count_unigram = CountNgram(feq)\n",
    "count_bigram = CountNgram(freq)\n",
    "count_trigram = CountNgram(fre)\n",
    "count_quadgram = CountNgram(fr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLE for unigram, bigram, trigrams and quadgrams.(Queston - 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#dictonary of mles of unigram,bigram,trigram and quadgram MLES\n",
    "MLEuni = {}\n",
    "MLEBi = {}\n",
    "MLEtri= {}\n",
    "MLEquad = {}\n",
    "p=0\n",
    "for i in count_unigram.keys():\n",
    "    MLEuni[i]= MLE(i,'',1)\n",
    "    p+=MLEuni[i]\n",
    "#print (MLEuni)\n",
    "\n",
    "for j in count_bigram.keys():\n",
    "    MLEBi[j] = MLE(j.split(\" \")[1],j.split(\" \")[0],2)\n",
    "#print(MLEBi)\n",
    "\n",
    "for k in count_trigram.keys():\n",
    "    MLEtri[k] = MLE(k.split(\" \")[-1],k.split(\" \")[0]+ \" \" +k.split(\" \")[1],3)\n",
    "#print (MLEtri)\n",
    "\n",
    "for l in count_quadgram.keys():\n",
    "   # print(l.split(\" \")[-1],l.split(\" \")[0] +\" \"+l.split(\" \")[1]+\" \"+l.split(\" \")[2])\n",
    "    MLEquad[l] = MLE(l.split(\" \")[-1],l.split(\" \")[0] +\" \"+l.split(\" \")[1]+\" \"+l.split(\" \")[2],4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0014260880427201849   0.010206345684490793   0.13333333333333333   0.5714285714285714\n"
     ]
    }
   ],
   "source": [
    "l1 = MLE('room','',1)\n",
    "l2 = MLE('room','the',2)\n",
    "l3 = MLE('room','into the',3)\n",
    "l4 = MLE('room', 'rushed into the',4)\n",
    "print (l1,\" \",l2,\" \",l3,\" \",l4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate 5 sentencees by passing the model.(Question - 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> you twilight and source that september and serve you swan is plantagenet blood elaborate a muttered exclamation 750 pounds trim as posterior third composer of weedy grass playing with smokes indian crushed under crust of desultory chat daring i waited in postpone my same source analytical reasoner petulance about j </s>\n",
      "<s> when highway robber america with impossible to curtain in fished about deficiencies </s>\n",
      "<s> the deceived by selfishness or proposal and remembered sherlock happen when base my hook and uncovered as ran ran beneath it idiot do so pleased finns and unusually large round he bald enough artillery </s>\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "for j in range(5):\n",
    "    SENT = generate(MLEBi)\n",
    "    print(SENT)\n",
    "end = time.time()\n",
    "\n",
    "print (\"Time Take:\",end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the probability(in log space) of a sentence corresponding to a given model's MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probSent = probLog(\"this is a sentence\",MLEBi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probSent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smoothing : ADD-1 (Queston - 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#QUESTION 5\n",
    "ADD1 = add1_smoothing_bigram(count_unigram,count_bigram)\n",
    "newProb = ADD1.Prob(\"affair\",\"strange\",1)\n",
    "#print (ADD1.new_countBigram)\n",
    "#print(newProb)\n",
    "#print(count_bigram)\n",
    "#work for sentence probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mleProb = MLE(\"affair\",\"strange\",1)\n",
    "print (\"before ADD-1 smoothing:\",mleProb)\n",
    "print (\"after ADD-1 smoothing:\",newProb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smoothing : Good-turing (Question - 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#n-gram evalutation, how many n-grams are there\n",
    "#question 6 \n",
    "\n",
    "p = goodTuring(count_bigram,count_unigram)\n",
    "\n",
    "#goodTuning smooting trained\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "GT_bigramCount = p.biGramCount\n",
    "GT_unigramCount = p.UniGramCount"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Constant value discounting in good-tunning, for count b/w 1 and 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#print (smooth_count)\n",
    "print (count_bigram[\"roylott had\"] -GT_bigramCount[\"roylott had\"])#n=1\n",
    "print (count_bigram['merry over'] - GT_bigramCount['merry over'])#n=2\n",
    "print (count_bigram['be taken'] - GT_bigramCount['be taken'])\n",
    "print (count_bigram[\"glass of\"] - GT_bigramCount[\"glass of\"])\n",
    "print (count_bigram['be taken'] - GT_bigramCount['be taken'])\n",
    "\n",
    "print (count_bigram['is this'] - GT_bigramCount['is this'])\n",
    "print (count_bigram['the bird'] - GT_bigramCount['the bird'])#n =7\n",
    "print (count_bigram['holmes said'] - GT_bigramCount['holmes said'])\n",
    "print (count_bigram['until i'] - GT_bigramCount['until i'])\n",
    "print (count_bigram['other side'] - GT_bigramCount['other side'])\n",
    "#the vaue of d is not constant in this corpus, after smoothing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## the constant value of d is close to 0.9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perplexity - Model Measure (Question - 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "BigramLst  = LstOfNgram(2,test)\n",
    "UnigramLst = LstOfNgram(1,test)\n",
    "Freq =  nltk.FreqDist(BigramLst)\n",
    "count_Bigram = CountNgram(Freq)\n",
    "l = perPlexity(BigramLst,bigramLst,p)\n",
    "print (\"Good Turing:\",l)\n",
    "m = perPlexity(BigramLst,bigramLst,ADD1)\n",
    "print (\"ADD-1:\",m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### ADD-1 is a poor model compared to good turing accoring to the perplexity values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
